{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model training for WSCNet2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import cv2\n",
    "import time\n",
    "import torch\n",
    "import numpy as np\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "\n",
    "from dropDataset import DropDataset, WSCDataset\n",
    "from dropNets import DropCondNet, WSCNet, WSCLoss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parameters\n",
    "# target_dir = 'D:/data/droplets' # Path to the directory where the data is stored (the top directory of the data)\n",
    "target_dir = 'G:/images/dropDatasetDrops/c3ra43624a_4/train' # Path to the directory where the data is stored (the top directory of the data)\n",
    "\n",
    "net_name = 'WSCNet' # Name of the network to be used, chosen from ['Resnet18', 'Resnet50', 'LeNet', 'MobileNet', 'WSCNet']\n",
    "\n",
    "pretrained_model_path = '' # Path to the pretrained model, '' if not using pretrained model\n",
    "\n",
    "batch_size = 64 # Batch size for training\n",
    "\n",
    "\n",
    "assert net_name in ['Resnet18', 'Resnet50', 'LeNet', 'MobileNet', 'WSCNet'], 'net_name must be chosen from [\"Resnet18\", \"Resnet50\", \"LeNet\", \"MobileNet\", \"WSCNet\"]'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Droplet segmentation\n",
    "\n",
    "Use modified results to generate droplet segmentation images, which will be saved in `<target_dir>/drops`.\n",
    "These images will soon be loaded for model training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create directory\n",
    "for i in range(4):\n",
    "    if not os.path.exists(os.path.join(target_dir, 'drops', str(i))):\n",
    "        os.makedirs(os.path.join(target_dir, 'drops', str(i)))\n",
    "    else: # clear\n",
    "        for file in os.listdir(os.path.join(target_dir, 'drops', str(i))):\n",
    "            os.remove(os.path.join(target_dir, 'drops', str(i), file))\n",
    "\n",
    "# search image files\n",
    "img_ext_list = ['.jpg', '.png', '.bmp', '.JPG', '.PNG', '.BMP']\n",
    "img_name_list = []\n",
    "for file in os.listdir(target_dir):\n",
    "    if file[-4:] in img_ext_list:\n",
    "        img_name_list.append(file)\n",
    "print('Number of images: ', len(img_name_list))\n",
    "\n",
    "# load modified results and save droplet images\n",
    "drop_num_list = [0, 0, 0, 0]\n",
    "for img_name in img_name_list:\n",
    "    img_path = os.path.join(target_dir, img_name)\n",
    "    src_img = cv2.imread(img_path)\n",
    "\n",
    "    modified_text_path = os.path.join(target_dir, 'textResult', img_name[:-4] + '_modified.txt')\n",
    "    if not os.path.exists(modified_text_path):\n",
    "        continue\n",
    "\n",
    "    modified_results = np.loadtxt(modified_text_path, delimiter='\\t', dtype=np.string_)[:, :4].astype(np.float32)\n",
    "    for line in modified_results:\n",
    "        x, y, r, _class = line\n",
    "        _class = int(_class) + 1\n",
    "\n",
    "        x1 = max(int(x - r), 0)\n",
    "        x2 = min(int(x + r), src_img.shape[1])\n",
    "        y1 = max(int(y - r), 0) \n",
    "        y2 = min(int(y + r), src_img.shape[0])\n",
    "\n",
    "        drop_img = src_img[y1:y2, x1:x2]\n",
    "        cv2.imwrite(os.path.join(target_dir, 'drops', str(_class), img_name[:-4] + \"_drops_\" + str(i) + img_name[-4:]), drop_img)\n",
    "        drop_num_list[_class] += 1\n",
    "print('Number of droplets in each class: ', drop_num_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Training preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = []\n",
    "for i in range(4):\n",
    "    path_i = os.path.join(target_dir, 'drops', str(i))\n",
    "    for file in os.listdir(path_i):\n",
    "        file_path = os.path.join(path_i, file)\n",
    "        data.append((file_path, i))\n",
    "\n",
    "np.random.shuffle(data)\n",
    "\n",
    "valid_rate = 0.2\n",
    "train_data = data[:int(len(data) * (1 - valid_rate))]\n",
    "valid_data = data[int(len(data) * (1 - valid_rate)):]\n",
    "if net_name == 'WSCNet':\n",
    "    train_dataset = DropDataset(train_data, mode='WSCNet')\n",
    "    valid_dataset = DropDataset(valid_data, mode='valid')\n",
    "else:\n",
    "    train_dataset = DropDataset(train_data, mode='train')\n",
    "    valid_dataset = DropDataset(valid_data, mode='valid')\n",
    "train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=batch_size, shuffle=True, num_workers=0)\n",
    "valid_loader = torch.utils.data.DataLoader(valid_dataset, batch_size=batch_size, shuffle=False, num_workers=0)\n",
    "print('Number of training droplets: ', len(train_dataset))\n",
    "print('Number of validation droplets: ', len(valid_dataset))\n",
    "\n",
    "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
    "print('Using device: ', device)\n",
    "\n",
    "if net_name == 'WSCNet':\n",
    "    net = WSCNet().to(device)\n",
    "else:\n",
    "    net = DropCondNet(net_name).to(device)\n",
    "# print('Network: ', net)\n",
    "\n",
    "if pretrained_model_path != '':\n",
    "    net.load_state_dict(torch.load(pretrained_model_path))\n",
    "    print('Pretrained model: ', pretrained_model_path)\n",
    "\n",
    "if net_name == 'WSCNet':\n",
    "    criterion = WSCLoss(device)\n",
    "else: # net_name in ['Resnet18', 'Resnet50', 'LeNet', 'MobileNet']\n",
    "    criterion = torch.nn.CrossEntropyLoss()\n",
    "\n",
    "optimizer = torch.optim.Adam(net.parameters(), lr=0.001) # optimizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Model training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "writer = SummaryWriter('runs/' + net_name + time.strftime('_%m%d%H%M', time.localtime(time.time())))\n",
    "\n",
    "print('Start training...')\n",
    "max_epoch = 1000\n",
    "min_valid_loss = 100000\n",
    "for epoch in range(max_epoch):\n",
    "    ## Train\n",
    "    running_loss = 0.0\n",
    "    net.train()\n",
    "    for i, data in enumerate(train_loader):\n",
    "        inputs,labels = data\n",
    "        inputs,labels = inputs.to(device),labels.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        outputs = net(inputs)\n",
    "        if net_name == 'WSCNet':\n",
    "            loss = criterion(*outputs,labels)\n",
    "        else:\n",
    "            loss = criterion(outputs,labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        running_loss += loss.cpu().item()\n",
    "\n",
    "    print('[%d] train loss: %.3f' %(epoch+1, 1000*running_loss/len(train_dataset)))\n",
    "    writer.add_scalar('train_loss', 1000*running_loss/len(train_dataset), epoch)\n",
    "\n",
    "    ## Valid\n",
    "    running_loss = 0.0\n",
    "    net.eval()\n",
    "    with torch.no_grad():\n",
    "        correct_num = [0, 0, 0, 0]\n",
    "        total_num = [0, 0, 0, 0]\n",
    "        for i, data in enumerate(valid_loader):\n",
    "            inputs,labels = data\n",
    "            inputs,labels = inputs.to(device),labels.to(device)\n",
    "            outputs = net(inputs)\n",
    "            if net_name == 'WSCNet':\n",
    "                loss = criterion(*outputs,labels)\n",
    "            else:\n",
    "                loss = criterion(outputs,labels)\n",
    "            running_loss += loss.cpu().item()\n",
    "            # calculate accuracy\n",
    "            if not net_name == 'WSCNet':\n",
    "                outputs_label = torch.argmax(outputs, dim=1)\n",
    "                inputs_label = torch.argmax(labels, dim=1)\n",
    "                outputs_label = outputs_label.cpu().numpy()\n",
    "                inputs_label = inputs_label.cpu().numpy()\n",
    "                for j in range(len(outputs_label)):\n",
    "                    if outputs_label[j] == inputs_label[j]:\n",
    "                        correct_num[inputs_label[j]] += 1\n",
    "                    total_num[inputs_label[j]] += 1\n",
    "\n",
    "    print('[%d] valid loss : %.3f' %(epoch+1, 1000*running_loss/len(valid_dataset)))\n",
    "    writer.add_scalar('valid_loss', 1000*running_loss/len(valid_dataset), epoch)\n",
    "    if not net_name == 'WSCNet':\n",
    "        print('[%d] valid acc : %.3f' %(epoch+1, 100*sum(correct_num)/sum(total_num)))\n",
    "        writer.add_scalar('valid_acc_total', 100*sum(correct_num)/sum(total_num), epoch)\n",
    "        for i in range(4):\n",
    "            if total_num[i] == 0:\n",
    "                continue\n",
    "            writer.add_scalar('valid_acc_' + str(i), 100*correct_num[i]/total_num[i], epoch)\n",
    "    print('----------------------------------------------')\n",
    "\n",
    "    ## Save best model\n",
    "    if running_loss < min_valid_loss:\n",
    "        min_valid_loss = running_loss\n",
    "        es_num = 0 # early stop number\n",
    "        model_save_path = os.path.join(target_dir, 'Drop_' + net_name + '.pt')\n",
    "        torch.save(net.state_dict(), model_save_path)\n",
    "    else:\n",
    "        es_num += 1\n",
    "\n",
    "    ## early stop\n",
    "    if es_num >= 50:\n",
    "        print('early stop')\n",
    "        break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Generate traced model for software using\n",
    "If you need to use trained model in WSCNet2.exe to inference, don't forget this step. Only traced model can be used in WSCNet2.exe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "net.load_state_dict(torch.load(os.path.join(target_dir, net_name + '.pt')))\n",
    "net.eval()\n",
    "\n",
    "input = torch.randn(1, 3, 32, 32).to(device)\n",
    "traced_script_module = torch.jit.trace(net, input)\n",
    "traced_script_module.save(os.path.join(target_dir, 'traced_' + net_name + '.pth'))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
